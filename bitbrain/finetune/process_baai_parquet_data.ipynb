{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "ROOT_DIR = \"/.cache/modelscope/datasets/BAAI/Infinity-Instruct/7M\"\n",
    "all_files = glob(ROOT_DIR+\"/*\")\n",
    "print(len(all_files), all_files)\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "model_path = \"/.cache/modelscope/models/Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_alpaca_data(str_json_data):\n",
    "    return {\n",
    "                \"instruction\": str_json_data[-2][\"value\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": str_json_data[-1][\"value\"]\n",
    "            }\n",
    "\n",
    "def to_share_gpt_data(str_json_data):\n",
    "    conversations = str_json_data.tolist()\n",
    "    text = \"\"\n",
    "    for item in conversations:\n",
    "        text += item[\"value\"]\n",
    "    return {\n",
    "        \"conversations\": conversations\n",
    "    }, text\n",
    "\n",
    "def process_one_data(file_dir):\n",
    "    table = pq.read_table(file_dir)\n",
    "    df = table.to_pandas()\n",
    "    # Index(['id', 'conversations', 'label', 'langdetect', 'source'], dtype='object')\n",
    "    # print(df.columns)\n",
    "    # 可以过滤=======================\n",
    "    # need_idx = df[\"langdetect\"] == \"zh-cn\"\n",
    "    # df = df.loc[need_idx, :]\n",
    "    #===============================\n",
    "    str_json_datas = df[\"conversations\"]\n",
    "    language_datas = df[\"langdetect\"]\n",
    "    muti_turn_counter = 0\n",
    "    odd_cov_counter = 0\n",
    "    encounter = 0\n",
    "    a_file_datas = []\n",
    "    text_list = []\n",
    "    for idx, str_json_data in  enumerate(str_json_datas):\n",
    "        L = len(str_json_data)\n",
    "        # #! 过滤出中文数据集\n",
    "        # if language_datas.iloc[idx] != \"zh-cn\":\n",
    "        #     encounter += 1\n",
    "        #     continue\n",
    "        #! 过滤多轮对话\n",
    "        if L > 2:\n",
    "            muti_turn_counter += 1\n",
    "        if L % 2 == 1:\n",
    "            odd_cov_counter += 1\n",
    "        # a_file_datas.append(to_alpaca_data(str_json_data)) \n",
    "        conversation, text= to_share_gpt_data(str_json_data)\n",
    "        text_list.append(text)\n",
    "        a_file_datas.append(conversation)\n",
    "    \n",
    "    input_ids = tokenizer(text_list)['input_ids']\n",
    "    token_num_list = [len(x) for x in input_ids]\n",
    "    print(f\"all data:{len(str_json_datas)} muti_turn_counter:{muti_turn_counter} odd_cov_counter:{odd_cov_counter} encounter:{encounter}\")\n",
    "    return a_file_datas,token_num_list\n",
    "\n",
    "all_datas = []\n",
    "token_nums = []\n",
    "for file_dir in tqdm(all_files):\n",
    "    datas,token_num_list = process_one_data(file_dir) \n",
    "    all_datas.extend(datas) \n",
    "    token_nums.extend(token_num_list)\n",
    "print(len(all_datas))\n",
    "print(len(token_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.percentile(token_nums,[50, 60, 70, 80, 90,95,99])\n",
    "print(np.percentile(token_nums,[50, 60, 70, 80, 90,95, 99]))\n",
    "filtered_long_data = []\n",
    "filtered_token_num = []\n",
    "for i in range(len(all_datas)):\n",
    "    if token_nums[i] < 768:\n",
    "        filtered_long_data.append(all_datas[i])\n",
    "        filtered_token_num.append(token_nums[i])\n",
    "print(np.percentile(filtered_token_num,[50, 60, 70, 80, 90,95, 99]))\n",
    "print(len(filtered_token_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = '/.cache/sft_data/llamafactory_input/BAAI_Infinity-Instruct_682W.jsonl'\n",
    "\n",
    "directory = os.path.dirname(file_path)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"目录{directory} 不存在，已创建\")\n",
    "else:\n",
    "    print(f\"目录{directory} 已存在\")\n",
    "\n",
    "with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "    for item in filtered_long_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
